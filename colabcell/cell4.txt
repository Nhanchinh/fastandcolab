# Cell 4: Load models

import torch
from transformers import (
    AutoTokenizer,
    AutoModelForSeq2SeqLM,
    AutoModelForCausalLM,
    AutoModel,
    BitsAndBytesConfig  # ThÃªm Ä‘á»ƒ quantize
)

device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"ğŸ–¥ï¸ Device: {device}")
if device == "cuda":
    print(f"   GPU: {torch.cuda.get_device_name(0)}")
    print(f"   VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")

# Load ViT5 Model (Finetuned + Logic Fixed)
print("\nğŸ“¦ Loading ViT5 (logic-fixed)...")
vit5_tokenizer = AutoTokenizer.from_pretrained("tncinh/vit5-logic-fixed")
vit5_model = AutoModelForSeq2SeqLM.from_pretrained("tncinh/vit5-logic-fixed").to(device)
print("   âœ… ViT5 loaded!")

# Load ViT5 Paraphrase Model
print("\nğŸ“¦ Loading ViT5 Paraphrase...")
paraphrase_tokenizer = AutoTokenizer.from_pretrained("tncinh/my-vit5-paraphrase")
paraphrase_model = AutoModelForSeq2SeqLM.from_pretrained("tncinh/my-vit5-paraphrase").to(device)
print("   âœ… ViT5 Paraphrase loaded!")

# Load PhoBERT Model
print("\nğŸ“¦ Loading PhoBERT...")
phobert_tokenizer = AutoTokenizer.from_pretrained("vinai/phobert-base")
phobert_model = AutoModel.from_pretrained("vinai/phobert-base").to(device)
print("   âœ… PhoBERT loaded!")

# Load Qwen 7B vá»›i 4-bit quantization Ä‘á»ƒ tiáº¿t kiá»‡m VRAM
print("\nğŸ“¦ Loading Qwen2.5-7B (4-bit quantized)...")

# Cáº¥u hÃ¬nh 4-bit quantization
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True  # Tiáº¿t kiá»‡m thÃªm VRAM
)

qwen_tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-7B-Instruct")
qwen_model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct",
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True
)
print("   âœ… Qwen 7B loaded! (4-bit quantized)")

# In VRAM usage
if device == "cuda":
    print(f"\nğŸ“Š VRAM Ä‘ang dÃ¹ng: {torch.cuda.memory_allocated() / 1024**3:.2f} GB")
    print(f"   VRAM cÃ²n trá»‘ng: {(torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated()) / 1024**3:.2f} GB")

print("\nğŸ‰ Táº¤T Cáº¢ MODELS ÄÃƒ Sáº´N SÃ€NG!")